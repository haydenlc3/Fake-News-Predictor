{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Predictor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaIBmnXCknPl"
      },
      "source": [
        "About the Dataset:\n",
        "\n",
        "1. id: unique id for a news article\n",
        "2. title: the title of a news article\n",
        "3. author: author of the news article\n",
        "4. text: the text of the article; could be incomplete\n",
        "5. label: a label that marks whether the news article is real or fake:\n",
        "           1: Fake news\n",
        "           0: real News\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Dependencies"
      ],
      "metadata": {
        "id": "4atg9aBjoa3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/codelucas/newspaper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rvgvoe4UibK",
        "outputId": "ae2e297f-430a-4111-a623-58a0d7abf579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/codelucas/newspaper.git\n",
            "  Cloning https://github.com/codelucas/newspaper.git to /tmp/pip-req-build-56n884r2\n",
            "  Running command git clone -q https://github.com/codelucas/newspaper.git /tmp/pip-req-build-56n884r2\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (4.6.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (0.0.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (6.0.10)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (0.35.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (4.2.6)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: pythainlp>=1.7.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (3.13)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (0.3)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k==0.3.0) (3.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp>=1.7.2->newspaper3k==0.3.0) (4.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k==0.3.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k==0.3.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp>=1.7.2->newspaper3k==0.3.0) (4.2.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k==0.3.0) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k==0.3.0) (3.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('stopwords')\n",
        "corpus = []"
      ],
      "metadata": {
        "id": "Q5Kx5_ZSoXpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce74c23-9354-4432-fb74-19f424cedf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-processing"
      ],
      "metadata": {
        "id": "2Lo93WSvq9kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset to a pandas DataFrame\n",
        "news_dataset = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "TiPLko87qDz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace null values with empty string\n",
        "news_dataset = news_dataset.fillna('')"
      ],
      "metadata": {
        "id": "0vQoH26Hr__G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging author name and news title\n",
        "news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']"
      ],
      "metadata": {
        "id": "7FjBwpuOu6e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the data & label\n",
        "# remove row: axis=0, remove column: axis=1\n",
        "X = news_dataset.drop(columns='label', axis=1) \n",
        "Y = news_dataset['label']"
      ],
      "metadata": {
        "id": "icSIYPN1vf3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming:\n",
        "The process of reducing a word to its root word"
      ],
      "metadata": {
        "id": "HAkg6LWEwx-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "port_stem = PorterStemmer()"
      ],
      "metadata": {
        "id": "nRRUMgxLw5Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(content):\n",
        "  stemmed_content = re.sub('[^a-zA-Z]', ' ', content).lower().split()\n",
        "  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "  stemmed_content = ' '.join(stemmed_content)\n",
        "  corpus.append(stemmed_content)\n",
        "  return stemmed_content"
      ],
      "metadata": {
        "id": "4LhQmZIOxajt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset['content'] = news_dataset['content'].apply(stemming)\n",
        "print(news_dataset['content'][0])"
      ],
      "metadata": {
        "id": "bsjKplY-1Mrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349d5777-2495-496a-f3fc-4d9e1c93924a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "darrel lucu hou dem aid even see comey letter jason chaffetz tweet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the data and label\n",
        "Y = news_dataset['label'].values"
      ],
      "metadata": {
        "id": "n6_ioP-A1-3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting text data to number data\n",
        "vectorizer = TfidfVectorizer().fit(corpus)\n",
        "X = vectorizer.transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "5FTZzhPW3OW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model: Logistic Regression"
      ],
      "metadata": {
        "id": "aI4Ri4tWAvfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "model = LogisticRegression().fit(X, Y)"
      ],
      "metadata": {
        "id": "iYEwEnAkAITa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation: Accuracy score"
      ],
      "metadata": {
        "id": "0mX_q8ECB1zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score on the training data\n",
        "X_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(X_prediction, Y_train)\n",
        "print('Accuracy of the training data : ', training_data_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIpYrtOSA1ZV",
        "outputId": "a96cfdc8-8dc0-49ed-ea3a-3f37e6201fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the training data :  0.9886418269230769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-process Article Data"
      ],
      "metadata": {
        "id": "hq5AkzoCFaGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract data from article\n",
        "url = 'https://apnews.com/article/caribbean-caracas-venezuela-5a113f7f603f4d449e926ac9b981d4d5'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.html\n",
        "article.parse()\n",
        "print(article.authors)\n",
        "print(article.title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMp5J9igZOls",
        "outputId": "f539276f-f3f8-4202-820a-edf64f92ca85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Regina Garcia Cano']\n",
            "On Venezuelan roads, old cars prevail, break down everywhere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_authors = ''\n",
        "for author in article.authors:\n",
        "  article_authors = article_authors + author + ' '\n",
        "article_data = article_authors + ' ' + article.title\n",
        "print(article_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnhkmuH9e3Bd",
        "outputId": "e607d917-cd69-4263-9c37-b0a862b9c8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regina Garcia Cano  On Venezuelan roads, old cars prevail, break down everywhere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = [stemming(article_data)]\n",
        "print(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BejfXUEsjpUe",
        "outputId": "b0b170d6-7425-4e0e-f49f-edfaf7af1f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['regina garcia cano venezuelan road old car prevail break everywher']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = vectorizer.transform(X_new).toarray()"
      ],
      "metadata": {
        "id": "L0SKDoaPk1DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a Prediction"
      ],
      "metadata": {
        "id": "HRv0LlbLdRDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X_new)\n",
        "\n",
        "if (prediction == 0):\n",
        "  print('The news is real')\n",
        "else:\n",
        "  print('The news is fake')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3zpljmeFTMA",
        "outputId": "1e039c64-de3f-4194-e5c5-d6876c0758e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The news is fake\n"
          ]
        }
      ]
    }
  ]
}